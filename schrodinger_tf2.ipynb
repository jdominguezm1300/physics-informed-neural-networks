{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "schrodinger_14.07.2021.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOsK1d/Q8lFZ1C20KH8zlNv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdominguezm1300/physics-informed-neural-networks/blob/main/schrodinger_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gXPNvgTu8-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7903271-dac6-497b-8d11-9da1584c66f2"
      },
      "source": [
        "!pip install pyDOE\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyDOE\n",
            "  Downloading pyDOE-0.3.8.zip (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyDOE) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyDOE) (1.4.1)\n",
            "Building wheels for collected packages: pyDOE\n",
            "  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyDOE: filename=pyDOE-0.3.8-py3-none-any.whl size=18184 sha256=ef56b05115931681c3e689b68b916cc646c7458c347a5ef899330f149624ed52\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/ce/8a/87b25c685bfeca1872d13b8dc101e087a9c6e3fb5ebb47022a\n",
            "Successfully built pyDOE\n",
            "Installing collected packages: pyDOE\n",
            "Successfully installed pyDOE-0.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m8n5-c3vd8M"
      },
      "source": [
        "#Liberias requeridas\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy import optimize\n",
        "from scipy.interpolate import griddata\n",
        "from pyDOE import lhs\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from datetime import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lueps3mhwC78"
      },
      "source": [
        "class dummy(object):\n",
        "  pass\n",
        "\n",
        "class Struct(dummy):\n",
        "  def __getattribute__(self, key):\n",
        "    if key == '__dict__':\n",
        "      return super(dummy, self).__getattribute__('__dict__')\n",
        "    return self.__dict__.get(key, 0)\n",
        "#Clase del modelo de red profunda informada por la física\n",
        "class PhysicsInformedNN():\n",
        "  #__init__\n",
        "  #Descripción: Constructor de la clase del modelo de la red neuronal profunda\n",
        "  def __init__(self,N0,N_b,N_f,layers,tf_epochs,tf_lr,tf_b1,tf_eps,nt_epochs,nt_lr,nt_ncorr,log_frequency, X_f,tb,ub,lb):\n",
        "    #X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
        "    self.X_lb =tf.convert_to_tensor(np.concatenate((0*tb + lb[0], tb), 1),dtype=tf.float64) # (lb[0], tb)\n",
        "    self.X_ub =tf.convert_to_tensor(np.concatenate((0*tb + ub[0], tb), 1),dtype=tf.float64) # (ub[0], tb)\n",
        "    self.x_f = tf.convert_to_tensor(X_f[:,0:1], dtype=tf.float64)\n",
        "    self.t_f = tf.convert_to_tensor(X_f[:,1:2], dtype=tf.float64)\n",
        "    #Inicialización de los Hiperparámetros de la red neuronal profunda\n",
        "    self.nt_config = Struct()\n",
        "    self.nt_config.learningRate = nt_lr\n",
        "    self.nt_config.maxIter = nt_epochs\n",
        "    self.nt_config.nCorrection = nt_ncorr\n",
        "    self.nt_config.tolFun = 1.0 * np.finfo(float).eps\n",
        "    self.tf_epochs = tf_epochs\n",
        "    #Inicialización del optimizador Adam\n",
        "    self.tf_optimizer = tf.keras.optimizers.Adam(learning_rate=tf_lr,beta_1=tf_b1,epsilon=tf_eps)\n",
        "    tf.keras.backend.set_floatx('float64')\n",
        "    #Creación del Modelo de la red neuronal profunda\n",
        "    self.model = tf.keras.Sequential()\n",
        "    #Inicialización de la capa de entrada\n",
        "    self.model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "    #Inicialización de las capas ocultas\n",
        "    self.model.add(tf.keras.layers.Lambda(lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
        "    for width in layers[1:-1]:\n",
        "      self.model.add(tf.keras.layers.Dense(width, activation=tf.nn.tanh,kernel_initializer=\"glorot_normal\"))\n",
        "      self.model.add(tf.keras.layers.Dense(layers[-1], activation=None,kernel_initializer=\"glorot_normal\"))\n",
        "    # Creación de arreglos del tamaño de  weights/biases para su descomposición\n",
        "    self.sizes_w = []\n",
        "    self.sizes_b = []\n",
        "    self.epoch_nt=tf_epochs\n",
        "    self.loss_train_tf=[]\n",
        "    self.loss_train_nt=[]\n",
        "    self.start_time = time.time()\n",
        "    self.prev_time = self.start_time\n",
        "    self.frequency = log_frequency\n",
        "\n",
        "    \n",
        "  #net_uv\n",
        "  #Descripción: Neurona para el calculo de u(t,x) y v(t,x)\n",
        "  def net_uv(self,X):\n",
        "    x = X[:, 0:1]\n",
        "    t = X[:, 1:2]\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      tape.watch(x)\n",
        "      tape.watch(t)\n",
        "      Xtemp = tf.concat([x,t],axis=1)\n",
        "      uv = self.model(Xtemp)\n",
        "      u = uv[:,0:1]\n",
        "      v = uv[:,1:2]\n",
        "    #Calculo de los gradientes\n",
        "    u_x = tape.gradient(u, x)\n",
        "    v_x = tape.gradient(v, x)\n",
        "    del tape\n",
        "    return u, v, u_x, v_x\n",
        "\n",
        "  #net_f_uv\n",
        "  #Decrioción: Neurona para el calculo de f(t,x)\n",
        "  def net_f_uv(self):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      tape.watch(self.x_f)\n",
        "      tape.watch(self.t_f)\n",
        "      X_f = tf.concat([self.x_f, self.t_f], axis=1)\n",
        "      u, v, u_x, v_x = self.net_uv(X_f)\n",
        "    #Calculo de gradientes\n",
        "    u_t = tape.gradient(u, self.t_f)\n",
        "    v_t = tape.gradient(v, self.t_f)\n",
        "    u_xx = tape.gradient(u_x, self.x_f)\n",
        "    v_xx = tape.gradient(v_x, self.x_f)\n",
        "    del tape\n",
        "    f_u = u_t + 0.5*v_xx + (u**2 + v**2)*v\n",
        "    f_v = v_t - 0.5*u_xx - (u**2 + v**2)*u   \n",
        "    return f_u, f_v\n",
        "  #loss\n",
        "  #Descripción: Función de perdida, calcula el error cuadratico medio\n",
        "  #             El eror MSE es la suma del error en MSE0,MSEB y MSF\n",
        "  def loss(self,uv,uv_pred ):\n",
        "    u0 = uv[:, 0:1]\n",
        "    v0 = uv[:, 1:2]\n",
        "    u0_pred = uv_pred[:, 0:1]\n",
        "    v0_pred = uv_pred[:, 1:2]\n",
        "    u_lb_pred, v_lb_pred, u_x_lb_pred, v_x_lb_pred = self.net_uv(self.X_lb)\n",
        "    u_ub_pred, v_ub_pred, u_x_ub_pred, v_x_ub_pred = self.net_uv(self.X_ub)\n",
        "    f_u_pred, f_v_pred = self.net_f_uv()\n",
        "    mse_0 = tf.reduce_mean(tf.square(u0 - u0_pred)) + tf.reduce_mean(tf.square(v0 - v0_pred))\n",
        "    mse_b = tf.reduce_mean(tf.square(u_lb_pred - u_ub_pred)) + tf.reduce_mean(tf.square(v_lb_pred - v_ub_pred)) + tf.reduce_mean(tf.square(u_x_lb_pred - u_x_ub_pred)) + tf.reduce_mean(tf.square(v_x_lb_pred - v_x_ub_pred))\n",
        "    mse_f = tf.reduce_mean(tf.square(f_u_pred)) + tf.reduce_mean(tf.square(f_v_pred))\n",
        "    loss_value=mse_0 + mse_b + mse_f\n",
        "    return loss_value\n",
        "\n",
        "  #predict\n",
        "  #Descripción: Función de predicción de la solución de la ecuación \n",
        "  #             de schrodinger con la red profunda entrenada\n",
        "  def predict(self, X_star):\n",
        "    h_pred = self.model(X_star)\n",
        "    u_pred = h_pred[:, 0:1]\n",
        "    v_pred = h_pred[:, 1:2]\n",
        "    return u_pred.numpy(), v_pred.numpy()\n",
        "    \n",
        "  #tf_optimization\n",
        "  #Descripción: Optimización con Adam\n",
        "  def tf_optimization(self, X_u, u):\n",
        "    print(f\"-- Starting Adam optimization --\")\n",
        "    for epoch in range(self.tf_epochs):\n",
        "      loss_value = self.tf_optimization_step(X_u, u)\n",
        "      self.loss_train_tf.append([epoch,loss_value.numpy()])\n",
        "      self.log_train_epoch(epoch, loss_value,is_iter=False)\n",
        "  \n",
        "  #tf_optimization_step\n",
        "  #Descripción: La función regresa el error obtenido despúes de aplicar \n",
        "  #             el optimizador Adam en una epoca\n",
        "  def tf_optimization_step(self, X_u, u):\n",
        "    loss_value, grads = self.grad(X_u, u)\n",
        "    self.tf_optimizer.apply_gradients(zip(grads,self.model.trainable_variables ))\n",
        "    return loss_value\n",
        "\n",
        "  #grad\n",
        "  #Descripción: La función regresa el error obtenido y \n",
        "  #             el caculo de los gradientes de las variables entrenables\n",
        "  def grad(self, X, u):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = self.loss(u, self.model(X))\n",
        "      grads = tape.gradient(loss_value, self.model.trainable_variables)\n",
        "    return loss_value, grads\n",
        "\n",
        "  #log_train_epoch\n",
        "  #Descripción:La función imprime la epoca de entrenamiendo, el tiempo transcurrido y el error\n",
        "  def log_train_epoch(self,epoch, loss, is_iter,custom=\"\"):\n",
        "    if epoch % self.frequency == 0:\n",
        "      name = 'nt_epoch' if is_iter else 'tf_epoch'\n",
        "      print(f\"{name} = {epoch:6d}  \" + f\"elapsed = {self.get_elapsed()} \" + f\"(+{self.get_epoch_duration()})  \" + f\"loss = {loss:.4e}  \" + custom)\n",
        "  \n",
        "  #get_epoch_duration\n",
        "  #Descripción: La función regresa el tiempo de entrenamiento de una epoca\n",
        "  def get_epoch_duration(self):\n",
        "        now = time.time()\n",
        "        edur = datetime.fromtimestamp(now - self.prev_time) \\\n",
        "            .strftime(\"%S.%f\")[:-5]\n",
        "        self.prev_time = now\n",
        "        return edur\n",
        "\n",
        "  #get_elapsed\n",
        "  #Descripción: La función regresa el tiempo transcurrido en una epoca \n",
        "  def get_elapsed(self):\n",
        "    return datetime.fromtimestamp(time.time() - self.start_time).strftime(\"%M:%S\")          \n",
        "\n",
        "  #set_weights\n",
        "  #Descripción: La función actualiza los pesos sinapticos de la red\n",
        "  def set_weights(self, w):\n",
        "    for i, layer in enumerate(self.model.layers[1:]):\n",
        "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
        "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
        "      weights = w[start_weights:end_weights]\n",
        "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
        "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
        "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
        "      weights_biases = [weights, biases]\n",
        "      layer.set_weights(weights_biases)\n",
        "\n",
        "  #get_weights\n",
        "  #Descripción:La función obtiene los pesos sinapticos de las capas de la red con sus bias\n",
        "  def get_weights(self):\n",
        "    w = []\n",
        "    for layer in self.model.layers[1:]:\n",
        "      weights_biases = layer.get_weights()\n",
        "      weights = weights_biases[0].flatten()\n",
        "      biases = weights_biases[1]\n",
        "      self.sizes_w.append(len(weights))\n",
        "      self.sizes_b.append(len(biases))\n",
        "      w.extend(weights)\n",
        "      w.extend(biases)\n",
        "    w = tf.convert_to_tensor(w, dtype=tf.float64)\n",
        "    return w\n",
        "\n",
        "  #get_loss_and_flat_grad\n",
        "  #Descripción:La función regresa el error y los gradiantes de las variables entrenables\n",
        "  def get_loss_and_flat_grad(self, X, u):\n",
        "    #loss_and_flat_grad\n",
        "    #Entrada: La función recibe un vector de pesos\n",
        "    #Descripción: La función llama a set_weights para actualizar los pesos,\n",
        "    #             despúes obtiene el error y calcula los gradientes de las variables entrenables\n",
        "    def loss_and_flat_grad(w):\n",
        "      grad_flat = []\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.set_weights(w)\n",
        "        loss_value = self.loss(u, self.model(X))\n",
        "        self.loss_train_nt.append([self.epoch_nt,loss_value.numpy()])\n",
        "      grad = tape.gradient(loss_value, self.model.trainable_variables)\n",
        "      #Se ordenan los gradientes en un tensor\n",
        "      for g in grad:\n",
        "        grad_flat.append(tf.reshape(g, [-1]))\n",
        "      grad_flat = tf.concat(grad_flat, 0)\n",
        "      self.log_train_epoch(self.epoch_nt, loss_value,is_iter=True)\n",
        "      self.epoch_nt+=1\n",
        "      return loss_value, grad_flat\n",
        "    return loss_and_flat_grad\n",
        "\n",
        "  #nt_optimization\n",
        "  #Descripción: La función realiza la optimización con LBFGS\n",
        "  #             la función que regresa el error y los gradiantes es loss_and_flat_grad en una tupla\n",
        "  #             y la posición inicial del optimizador son los pesos resultantes de la optimización con Adam  \n",
        "  def nt_optimization(self, X_u, u):\n",
        "    print(f\"-- Starting LBFGS optimization --\")\n",
        "    loss_and_flat_grad = self.get_loss_and_flat_grad(X_u, u)\n",
        " \n",
        "    results=tfp.optimizer.lbfgs_minimize(\n",
        "        loss_and_flat_grad,\n",
        "        initial_position=self.get_weights(),\n",
        "        num_correction_pairs=self.nt_config.nCorrection,\n",
        "        max_iterations=self.nt_config.maxIter,\n",
        "        f_relative_tolerance=self.nt_config.tolFun,\n",
        "        tolerance=self.nt_config.tolFun,\n",
        "        parallel_iterations=6)\n",
        "    print('L-BFGS Results')\n",
        "    print('Converged:', results.converged)\n",
        "    print('Location of the minimum:', results.position)\n",
        "    print('Number of iterations:', results.num_iterations)\n",
        "   \n",
        "  #fit\n",
        "  #Descripción: La función realiza el entrenamiento de la red con \n",
        "  #             los optimizadores Adam y LBFGS\n",
        "  def fit(self, X_u, u):\n",
        "    print(\"\\nTraining started\")\n",
        "    print(\"================\")\n",
        "    print(self.model.summary())\n",
        "    # Creating the tensors\n",
        "    X_u = tf.convert_to_tensor(X_u,dtype=tf.float64)\n",
        "    u =tf.convert_to_tensor(u,dtype=tf.float64)\n",
        "    # Optimizing\n",
        "    self.tf_optimization(X_u, u)\n",
        "    self.nt_optimization(X_u, u)\n",
        "    print(\"==================\")\n",
        "    print(f\"Training finished (epochs {self.tf_epochs+self.nt_config.maxIter}): \" + f\"duration = {self.get_elapsed()}  \"  )\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZIQxvUTgpoH"
      },
      "source": [
        "#Se establecen los Hiperparametros de la red\n",
        "N0 = 50\n",
        "N_b = 50\n",
        "N_f = 20000\n",
        "#Capas\n",
        "layers = [2, 100, 100, 100, 100, 2]\n",
        "#Epocas\n",
        "tf_epochs=1000\n",
        "tf_lr=0.05\n",
        "tf_b1=0.99\n",
        "tf_eps=1e-1\n",
        "nt_epochs=1000\n",
        "nt_lr=1.2\n",
        "nt_ncorr=50\n",
        "#Frecuencia para imprimir los resultados\n",
        "log_frequency=10\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh7YmBKjgyzM"
      },
      "source": [
        "#Carga de los datos\n",
        "noise=0.0\n",
        "data = scipy.io.loadmat('NLS.mat')\n",
        "lb = np.array([-5.0, 0.0])\n",
        "ub = np.array([5.0, np.pi/2])\n",
        "t = data['tt'].flatten()[:,None]\n",
        "x = data['x'].flatten()[:,None]\n",
        "Exact = data['uu']\n",
        "Exact_u = np.real(Exact)\n",
        "Exact_v = np.imag(Exact)\n",
        "Exact_h = np.sqrt(Exact_u**2 + Exact_v**2)\n",
        "X, T = np.meshgrid(x,t)\n",
        "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "u_star = Exact_u.T.flatten()[:,None]\n",
        "v_star = Exact_v.T.flatten()[:,None]\n",
        "h_star = Exact_h.T.flatten()[:,None]    \n",
        "idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
        "x0 = x[idx_x,:]\n",
        "u0 = Exact_u[idx_x,0:1]\n",
        "v0 = Exact_v[idx_x,0:1]    \n",
        "idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
        "tb = t[idx_t,:]\n",
        "X_f = lb + (ub-lb)*lhs(2, N_f)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC0yHQkbxLyp",
        "outputId": "bae081d0-14b1-4702-d08f-c394a2b3b156"
      },
      "source": [
        "pinn = PhysicsInformedNN(N0,N_b,N_f,layers,tf_epochs,tf_lr,tf_b1,tf_eps,nt_epochs,nt_lr,nt_ncorr,log_frequency, X_f,tb,ub,lb)\n",
        "pinn.fit(x0, tf.concat([u0, v0], axis=1))\n",
        "#Se realiza la predicción una vez entrenada la red\n",
        "u_pred, v_pred = pinn.predict(X_star)\n",
        "h_pred = np.sqrt(u_pred**2 + v_pred**2)\n",
        "#Se calcula el erro en la predicción\n",
        "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
        "error_v = np.linalg.norm(v_star-v_pred,2)/np.linalg.norm(v_star,2)\n",
        "error_h = np.linalg.norm(h_star-h_pred,2)/np.linalg.norm(h_star,2)\n",
        "\n",
        "print('Error u: %e' % (error_u))\n",
        "print('Error v: %e' % (error_v))\n",
        "print('Error h: %e' % (error_h))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training started\n",
            "================\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda (Lambda)              (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               300       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 202       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               300       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 202       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               300       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 2)                 202       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 100)               300       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 2,008\n",
            "Trainable params: 2,008\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "-- Starting Adam optimization --\n",
            "tf_epoch =      0  elapsed = 00:03 (+03.5)  loss = 9.3066e-01  \n",
            "tf_epoch =     10  elapsed = 00:29 (+26.2)  loss = 6.5359e-01  \n",
            "tf_epoch =     20  elapsed = 00:55 (+26.1)  loss = 5.3123e-01  \n",
            "tf_epoch =     30  elapsed = 01:21 (+25.9)  loss = 5.8247e-01  \n",
            "tf_epoch =     40  elapsed = 01:48 (+26.2)  loss = 5.1904e-01  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_Mb-Txh1OQ8"
      },
      "source": [
        "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
        "V_pred = griddata(X_star, v_pred.flatten(), (X, T), method='cubic')\n",
        "H_pred = griddata(X_star, h_pred.flatten(), (X, T), method='cubic')\n",
        "\n",
        "X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
        "X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
        "X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
        "X_u_train = np.vstack([X0, X_lb, X_ub])\n",
        "#Se grafican los resultados\n",
        "fig = plt.figure()\n",
        "ax = plt.axis('off')\n",
        "gs0 = gridspec.GridSpec(1, 2)\n",
        "gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
        "ax = plt.subplot(gs0[:, :])\n",
        "h = ax.imshow(H_pred.T, interpolation='nearest', cmap='YlGnBu', \n",
        "                  extent=[lb[1], ub[1], lb[0], ub[0]], \n",
        "                  origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "fig.colorbar(h, cax=cax)\n",
        "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (X_u_train.shape[0]), markersize = 4, clip_on = False)\n",
        "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "ax.plot(t[75]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
        "ax.plot(t[100]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
        "ax.plot(t[125]*np.ones((2,1)), line, 'k--', linewidth = 1)    \n",
        "ax.set_xlabel('$t$')\n",
        "ax.set_ylabel('$x$')\n",
        "leg = ax.legend(frameon=False, loc = 'best')\n",
        "ax.set_title('$|h(t,x)|$', fontsize = 10)\n",
        "gs1 = gridspec.GridSpec(1, 3)\n",
        "gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5) \n",
        "ax = plt.subplot(gs1[0, 0])\n",
        "ax.plot(x,Exact_h[:,75], 'b-', linewidth = 2, label = 'Exact')       \n",
        "ax.plot(x,H_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$|h(t,x)|$')    \n",
        "ax.set_title('$t = %.2f$' % (t[75]), fontsize = 10)\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-5.1,5.1])\n",
        "ax.set_ylim([-0.1,5.1])\n",
        "ax = plt.subplot(gs1[0, 1])\n",
        "ax.plot(x,Exact_h[:,100], 'b-', linewidth = 2, label = 'Exact')       \n",
        "ax.plot(x,H_pred[100,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$|h(t,x)|$')\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-5.1,5.1])\n",
        "ax.set_ylim([-0.1,5.1])\n",
        "ax.set_title('$t = %.2f$' % (t[100]), fontsize = 10)\n",
        "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.8), ncol=5, frameon=False)\n",
        "ax = plt.subplot(gs1[0, 2])\n",
        "ax.plot(x,Exact_h[:,125], 'b-', linewidth = 2, label = 'Exact')       \n",
        "ax.plot(x,H_pred[125,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$|h(t,x)|$')\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-5.1,5.1])\n",
        "ax.set_ylim([-0.1,5.1])    \n",
        "ax.set_title('$t = %.2f$' % (t[125]), fontsize = 10)\n",
        "plt.savefig('NLS')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsUPdL5Gl1X6"
      },
      "source": [
        "df_tf=pd.DataFrame(pinn.loss_train_tf)\n",
        "df_nt=pd.DataFrame(pinn.loss_train_nt)\n",
        "epochs_tf = df_tf[0]\n",
        "loss_tf = df_tf[1]\n",
        "epochs_nt = df_nt[0]\n",
        "loss_nt = df_nt[1]\n",
        "plt.plot(epochs_tf,loss_tf, label=\"ADAM\")\n",
        "plt.plot(epochs_nt,loss_nt, label=\"LBFGS\")\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('optimizacion')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}